{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10848602,"sourceType":"datasetVersion","datasetId":6737573},{"sourceId":282967,"sourceType":"modelInstanceVersion","modelInstanceId":242466,"modelId":264095},{"sourceId":284475,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":243784,"modelId":265402}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#загружаем нужные версии\n!pip uninstall -y torch torchtext\n!pip install torch==2.3.0\n!pip install torchtext==0.18\n!pip install sacrebleu","metadata":{"id":"foS2OKHkEceS","outputId":"df3285b1-9911-480e-adb0-0017b9967a3f","trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:32:17.278505Z","iopub.execute_input":"2025-03-14T10:32:17.278702Z","iopub.status.idle":"2025-03-14T10:34:51.360912Z","shell.execute_reply.started":"2025-03-14T10:32:17.278675Z","shell.execute_reply":"2025-03-14T10:34:51.359987Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.5.1+cu121\nUninstalling torch-2.5.1+cu121:\n  Successfully uninstalled torch-2.5.1+cu121\n\u001b[33mWARNING: Skipping torchtext as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mCollecting torch==2.3.0\n  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0) (2024.12.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.3.0 (from torch==2.3.0)\n  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.85)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0) (1.3.0)\nDownloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\ntorchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 triton-2.3.0\nCollecting torchtext==0.18\n  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.18) (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.18) (2.32.3)\nRequirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.18) (2.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.18) (1.26.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (12.1.105)\nRequirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext==0.18) (2.3.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext==0.18) (12.6.85)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext==0.18) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchtext==0.18) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchtext==0.18) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchtext==0.18) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchtext==0.18) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchtext==0.18) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchtext==0.18) (2024.2.0)\nDownloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torchtext\nSuccessfully installed torchtext-0.18.0\nCollecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.1.1 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Основные библиотеки PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch import Tensor\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.optim.lr_scheduler import LambdaLR\n\n# Токенизация и работа с текстом\nimport sentencepiece as spm\nfrom sentencepiece import SentencePieceTrainer, SentencePieceProcessor\n\n# Обработка данных\nimport numpy as np\nimport pandas as pd\nimport re\nimport os\nfrom typing import Union, List, Tuple, Type, Optional, Any\n\n# Утилиты\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n","metadata":{"id":"3yxgI-chEaSJ","outputId":"a5730907-cae1-4beb-d99e-1933632202cd","trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:34:51.361989Z","iopub.execute_input":"2025-03-14T10:34:51.362325Z","iopub.status.idle":"2025-03-14T10:34:54.571819Z","shell.execute_reply.started":"2025-03-14T10:34:51.362298Z","shell.execute_reply":"2025-03-14T10:34:54.571168Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \nTorchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \nTorchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport sentencepiece as spm\nfrom sentencepiece import SentencePieceTrainer, SentencePieceProcessor\n\nclass TextDataset(Dataset):\n    def __init__(self, german_file: str, english_file: str = None, \n                 tokenizer_prefix: str = \"tokenizer_\", vocab_capacity: int = 12000, \n                 token_model: str = \"word\", norm_rule: str = \"nmt_nfkc_cf\", \n                 seq_max_len: int = 128):\n        \"\"\"\n        german_file: путь к файлу с предложениями на немецком\n        english_file: путь к файлу с предложениями на английском (целевой язык)\n        tokenizer_prefix: префикс имени файлов для сохранения обученных моделей токенизаторов\n        vocab_capacity: размер vocab (количество токенов) для SentencePiece\n        token_model: тип модели токенизатора ('word', 'bpe', 'unigram', etc.)\n        norm_rule: правило нормализации (по умолчанию 'nmt_nfkc_cf' для Unicode NFKC)\n        seq_max_len: ограничение на максимальную длину последовательности (с учетом BOS/EOS)\n        \"\"\"\n        # Если модели токенизаторов не сохранены, обучаем их на тексте корпусов\n        if not os.path.isfile(tokenizer_prefix + 'german.model'):\n            # Обучаем SentencePiece для немецкого корпуса\n            SentencePieceTrainer.train(\n                input=german_file, model_prefix=tokenizer_prefix + 'german',\n                vocab_size=vocab_capacity, model_type=token_model,\n                normalization_rule_name=norm_rule,\n                pad_id=0, unk_id=1, bos_id=2, eos_id=3\n            )\n            # Обучаем SentencePiece для английского корпуса\n            SentencePieceTrainer.train(\n                input=english_file, model_prefix=tokenizer_prefix + 'english',\n                vocab_size=vocab_capacity, model_type=token_model,\n                normalization_rule_name=norm_rule,\n                pad_id=0, unk_id=1, bos_id=2, eos_id=3\n            )\n        # Загружаем обученные модели токенизаторов\n        self.tokenizer_de = SentencePieceProcessor(model_file=tokenizer_prefix + 'german.model')\n        self.tokenizer_en = SentencePieceProcessor(model_file=tokenizer_prefix + 'english.model')\n        \n        # Читаем все предложения из файлов\n        with open(german_file, encoding='utf-8') as f:\n            text_german = f.readlines()\n        total_sentences = len(text_german)\n        self.texts_de = text_german[:total_sentences]\n        self.indices_de = self.tokenizer_de.encode(self.texts_de)  # токенизируем весь немецкий корпус\n        \n        if english_file is not None:\n            with open(english_file, encoding='utf-8') as f:\n                text_english = f.readlines()\n            self.texts_en = text_english[:total_sentences]\n            self.indices_en = self.tokenizer_en.encode(self.texts_en)  # токенизируем весь английский корпус\n        else:\n            self.texts_en = None\n            self.indices_en = None\n        \n        # Сохраняем ID специальных токенов из токенизатора (должны совпадать с заданными выше)\n        self.pad_token = self.tokenizer_en.pad_id()    # == 0\n        self.unk_token = self.tokenizer_en.unk_id()    # == 1\n        self.start_token = self.tokenizer_en.bos_id()  # == 2 (BOS)\n        self.end_token = self.tokenizer_en.eos_id()    # == 3 (EOS)\n        self.seq_max_len = seq_max_len\n        # Размеры словарей исходного и целевого языков\n        self.vocab_size_de = self.tokenizer_de.vocab_size()\n        self.vocab_size_en = self.tokenizer_en.vocab_size()\n    \n    def __len__(self):\n        return len(self.texts_de)\n    \n    def _encode_text(self, text: str, lang: str):\n        \"\"\"Токенизировать строку с помощью соответствующего токенизатора\"\"\"\n        return (self.tokenizer_en.encode(text) if lang == 'English'\n                else self.tokenizer_de.encode(text))\n    \n    def _process_sentence(self, sentence: str, lang: str):\n        \"\"\"Добавить BOS, EOS и привести предложение к списку токенов (с обрезкой по seq_max_len).\"\"\"\n        tokens = [self.start_token] + self._encode_text(sentence, lang) + [self.end_token]\n        # Ограничиваем длину последовательности, если она превышает максимум\n        if len(tokens) > self.seq_max_len:\n            tokens = tokens[:self.seq_max_len]\n        return tokens\n    \n    def __getitem__(self, idx: int):\n        # Получаем список токенов для немецкого предложения (источник)\n        src_tokens = self._process_sentence(self.texts_de[idx].strip(), lang='German')\n        if self.texts_en is not None:\n            # Если есть целевой язык (обучение/валидация) - возвращаем пару (src, tgt)\n            tgt_tokens = self._process_sentence(self.texts_en[idx].strip(), lang='English')\n            return torch.tensor(src_tokens, dtype=torch.long), torch.tensor(tgt_tokens, dtype=torch.long)\n        else:\n            # Если target не задан (например, тестовый датасет) - возвращаем (src, пустой таргет)\n            dummy_target = [self.pad_token] * len(src_tokens)\n            return torch.tensor(src_tokens, dtype=torch.long), torch.tensor(dummy_target, dtype=torch.long)\n\ndef collate_fn(batch):\n    \"\"\"\n    Функция для формирования батча из списка пар (src, tgt),\n    дополняет последовательности паддингом до максимальной длины в батче.\n    \"\"\"\n    src_batch, tgt_batch = zip(*batch)\n    # Преобразуем список тензоров в один тензор [batch_size, seq_len] с padding\n    src_batch = pad_sequence(src_batch, padding_value=0, batch_first=True)\n    tgt_batch = pad_sequence(tgt_batch, padding_value=0, batch_first=True)\n    return src_batch, tgt_batch\n","metadata":{"id":"YErPi-5IDpz_","trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:34:54.572635Z","iopub.execute_input":"2025-03-14T10:34:54.573013Z","iopub.status.idle":"2025-03-14T10:34:54.585845Z","shell.execute_reply.started":"2025-03-14T10:34:54.572991Z","shell.execute_reply":"2025-03-14T10:34:54.585044Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import math\nimport torch.nn as nn\nfrom torch.nn import Transformer\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Модуль позиционного кодирования: добавляет информацию о позиции токена\nclass PositionalEncoding(nn.Module):\n    def __init__(self, emb_size: int, dropout: float = 0.1, maxlen: int = 5000):\n        super(PositionalEncoding, self).__init__()\n        # Вычисляем позиционные приучения для maxlen позиций заранее\n        pos = torch.arange(0, maxlen).unsqueeze(1)           # shape: [maxlen, 1]\n        i = torch.arange(0, emb_size, 2)                     # индексы 0,2,4,... для синусоид\n        # Формула из работы \"Attention is All You Need\": PE(pos, 2i) = sin(pos/10000^(2i/emb_size))\n        angle_rates = 1 / torch.pow(10000, (i.float()/emb_size))\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * angle_rates)  # синус для четных индексов\n        pos_embedding[:, 1::2] = torch.cos(pos * angle_rates)  # косинус для нечетных индексов\n        pos_embedding = pos_embedding.unsqueeze(1)  # shape: [maxlen, 1, emb_size]\n        self.register_buffer('pos_embedding', pos_embedding)  # не обучаемый параметр\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, token_embedding: torch.Tensor) -> torch.Tensor:\n        # token_embedding: shape [seq_len, batch_size, emb_size] (если batch_first=False)\n        # Добавляем позиционные векторы к эмбеддингам и применяем Dropout\n        seq_len = token_embedding.size(0)\n        return self.dropout(token_embedding + self.pos_embedding[:seq_len, :])\n\n# Модуль токен-эмбеддинга: обучаемый слой, преобразующий индексы в векторы\nclass TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, emb_size: int):\n        super(TokenEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.emb_size = emb_size\n    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n        # Возвращаем эмбеддинги, масштабированные на sqrt(emb_size) для стабильности\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n\n# Полная модель Seq2Seq с механизмом Transformer\nclass Seq2SeqTransformer(nn.Module):\n    def __init__(self, num_encoder_layers: int, num_decoder_layers: int,\n                 emb_size: int, nhead: int,\n                 src_vocab_size: int, tgt_vocab_size: int,\n                 dim_feedforward: int = 512, dropout: float = 0.1):\n        super(Seq2SeqTransformer, self).__init__()\n        # Инициализируем энкодер-декодер Transformer из PyTorch\n        self.transformer = Transformer(d_model=emb_size, nhead=nhead,\n                                       num_encoder_layers=num_encoder_layers,\n                                       num_decoder_layers=num_decoder_layers,\n                                       dim_feedforward=dim_feedforward,\n                                       dropout=dropout)\n        # Финальный линейный генератор вероятностей слов\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n        # Обучаемые эмбеддинги для источника и цели + позиционное кодирование\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n    \n    def forward(self, src: torch.Tensor, tgt: torch.Tensor, \n                src_mask: torch.Tensor, tgt_mask: torch.Tensor,\n                src_padding_mask: torch.Tensor, tgt_padding_mask: torch.Tensor,\n                memory_key_padding_mask: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Выполняет проход (encoder + decoder) и возвращает сырые логиты размером [tgt_seq_len, batch_size, tgt_vocab_size].\n        \"\"\"\n        # Получаем эмбеддинги входа и выхода с позиц.кодированием\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n        # Прогоняем через Transformer: \n        # src_mask - маска для энкодера (0 будущее), tgt_mask - маска будущего для декодера\n        # src_padding_mask/tgt_padding_mask - маски паддинга для энкодера и декодера\n        output = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, \n                                  None,  # нет явной маски между энкодером и декодером\n                                  src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n        logits = self.generator(output)  # Преобразуем скрытое представление в логиты слов\n        return logits\n    \n    def encode(self, src: torch.Tensor, src_mask: torch.Tensor):\n        \"\"\"Пропустить только через энкодер (для инференса).\"\"\"\n        return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)\n    \n    def decode(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor):\n        \"\"\"Пропустить только через декодер (для инференса, принимает уже полученную память энкодера).\"\"\"\n        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)\n","metadata":{"id":"L8zLRQpnDli_","trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:34:54.586688Z","iopub.execute_input":"2025-03-14T10:34:54.586991Z","iopub.status.idle":"2025-03-14T10:34:54.662514Z","shell.execute_reply.started":"2025-03-14T10:34:54.586957Z","shell.execute_reply":"2025-03-14T10:34:54.661667Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n    \"\"\"Создаёт маску размера sz x sz для запрета вниманию видеть последующие элементы.\"\"\"\n    mask = torch.triu(torch.ones(sz, sz, device=DEVICE)) == 1  # верхнетреугольная матрица из True\n    mask = mask.transpose(0, 1)  # приводим к нижнетреугольной (True на диагонали и ниже)\n    mask = mask.float().masked_fill(mask == False, float('-inf')).masked_fill(mask == True, float(0.0))\n    return mask\n\ndef create_mask(src: torch.Tensor, tgt: torch.Tensor):\n    src_seq_len = src.shape[0]   # длина последовательности источника (seq_len)\n    tgt_seq_len = tgt.shape[0]   # длина последовательности цели\n    # Маска будущих слов для декодера\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    # Маска для энкодера (нам не нужно ограничивать видимость, делаем булеву матрицу 0)\n    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n    # Маски заполнения (True там, где PAD) для входа и выхода (требуется форма [batch, seq_len])\n    src_padding_mask = (src == PAD_IDX).transpose(0, 1)  # [batch_size, src_seq_len]\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)  # [batch_size, tgt_seq_len]\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:34:54.664639Z","iopub.execute_input":"2025-03-14T10:34:54.664962Z","iopub.status.idle":"2025-03-14T10:34:54.685479Z","shell.execute_reply.started":"2025-03-14T10:34:54.664933Z","shell.execute_reply":"2025-03-14T10:34:54.684557Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train_epoch(model: nn.Module, optimizer, data_loader: DataLoader):\n    model.train()\n    total_loss = 0\n    for src, tgt in tqdm(data_loader, desc=\"Training\"):\n        src = src.to(DEVICE).transpose(0, 1)   # [seq_len, batch_size]\n        tgt = tgt.to(DEVICE).transpose(0, 1)   # [seq_len, batch_size]\n        # Разделяем целевые последовательности: \n        tgt_input = tgt[:-1, :]   # все кроме последнего токена (вход декодеру)\n        tgt_out   = tgt[1:, :]    # все кроме первого токена (ожидаемый выход)\n        # Генерируем маски\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n        # Прямой проход (forward)\n        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, \n                       memory_key_padding_mask=src_padding_mask)\n        # Вычисляем loss: приводим прогноз и целевые токены к размерности [-1] для функционала потерь\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        # Обратное распространение и шаг оптимизатора\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(data_loader)\n\ndef evaluate(model: nn.Module, data_loader: DataLoader):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for src, tgt in tqdm(data_loader, desc=\"Validation\"):\n            src = src.to(DEVICE).transpose(0, 1)\n            tgt = tgt.to(DEVICE).transpose(0, 1)\n            tgt_input = tgt[:-1, :]\n            tgt_out   = tgt[1:, :]\n            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n            logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask,\n                           memory_key_padding_mask=src_padding_mask)\n            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n            total_loss += loss.item()\n    return total_loss / len(data_loader)\n","metadata":{"id":"r9JrU6eDDhfc","trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:34:54.706579Z","iopub.execute_input":"2025-03-14T10:34:54.706869Z","iopub.status.idle":"2025-03-14T10:34:54.718243Z","shell.execute_reply.started":"2025-03-14T10:34:54.706838Z","shell.execute_reply":"2025-03-14T10:34:54.717528Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from timeit import default_timer as timer\nimport torch.optim.lr_scheduler as lr_scheduler\n\ntorch.manual_seed(0)\n\n# Параметры данных\nBATCH_SIZE = 128\n\n# Инициализируем датасеты для обучения и валидации\ntrain_dataset = TextDataset(german_file=\"train.de\", english_file=\"train.en\", \n                            tokenizer_prefix=\"tokenizer_\", is_training=True)\nval_dataset   = TextDataset(german_file=\"val.de\", english_file=\"val.en\", \n                            tokenizer_prefix=\"tokenizer_\", is_training=False)\n\n# DataLoader для итерации по батчам\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n# Специальные индексы (PAD, UNK, BOS, EOS) – должны соответствовать настройкам токенизатора\nPAD_IDX, UNK_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n\n# Размеры словарей исходного и целевого языков\nSRC_VOCAB_SIZE = train_dataset.vocab_size_de\nTGT_VOCAB_SIZE = train_dataset.vocab_size_en\n\n# Гиперпараметры модели\nEMB_SIZE = 512     # размер эмбеддингов и скрытого представления модели\nNHEAD = 8          # количество \"голов\" multi-head attention\nFFN_HID_DIM = 1024 # размер слоя feed-forward внутри трансформера\nNUM_ENCODER_LAYERS = 6\nNUM_DECODER_LAYERS = 6\n\n# Инициализируем модель\ntransformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, \n                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n# Инициализация параметров весов\nfor p in transformer.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n# Переносим модель на устройство (CPU/GPU)\ntransformer = transformer.to(DEVICE)\n\n# Функция потерь (кросс-энтропия), игнорируем индекс PAD\nloss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n\n# Оптимизатор Adam\noptimizer = torch.optim.Adam(transformer.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9, weight_decay=1e-5)\n\n# Планировщик OneCycleLR: один цикл изменения learning rate за все эпохи\nNUM_EPOCHS = 20\ntotal_steps = NUM_EPOCHS * len(train_loader)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-4, total_steps=total_steps, \n                                                pct_start=0.1, anneal_strategy='cos', final_div_factor=10)\n","metadata":{"id":"jBJFCy53FoAa","outputId":"ef47486c-b074-4ec2-ee89-c598d392a808","trusted":true,"execution":{"iopub.status.busy":"2025-03-14T10:34:54.752678Z","iopub.execute_input":"2025-03-14T10:34:54.752949Z","iopub.status.idle":"2025-03-14T10:35:07.298608Z","shell.execute_reply.started":"2025-03-14T10:34:54.752921Z","shell.execute_reply":"2025-03-14T10:35:07.297673Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from timeit import default_timer as timer\n\nfor epoch in range(1, NUM_EPOCHS + 1):\n    start_time = timer()\n    train_loss = train_epoch(transformer, optimizer, train_loader)\n    val_loss   = evaluate(transformer, val_loader)\n    end_time = timer()\n    scheduler.step()  # шаг изменения learning rate\n    \n    print(f\"Эпоха {epoch}: средний loss на обучении = {train_loss:.3f}, на валидации = {val_loss:.3f}, время эпохи = {end_time - start_time:.2f}с\")\n    # Сохраняем модель и оптимизатор\n    checkpoint_path = f\"model_epoch_{epoch}.pth\"\n    torch.save({\n        \"model_state_dict\": transformer.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"scheduler_state_dict\": scheduler.state_dict(),\n        \"epoch\": epoch,\n        \"val_loss\": val_loss\n    }, checkpoint_path)\n    print(f\"✔️ Модель сохранена в {checkpoint_path}\")\n","metadata":{"id":"wDRAgXVlGgnv","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:48:41.033757Z","iopub.execute_input":"2025-03-13T15:48:41.034086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def greedy_decode(model, src, src_mask, max_len, start_symbol):\n    model.eval()\n    # Прогоняем через энкодер\n    memory = model.encode(src, src_mask)\n    # Начинаем с последовательности, состоящей только из BOS\n    ys = torch.tensor([[start_symbol]], dtype=torch.long, device=DEVICE)  # shape [1,1]\n    for i in range(max_len - 1):\n        # Создаем маску для уже набранной последовательности (размер i+1)\n        tgt_mask = generate_square_subsequent_mask(ys.size(0)).to(DEVICE)\n        # Пропускаем через декодер текущую последовательность (ys) и память энкодера\n        out = model.decode(ys, memory, tgt_mask)\n        out = out.transpose(0, 1)  # теперь размер [batch=1, seq_len, emb_size]\n        # Прогнозируем следующий токен через генератор модели\n        prob = model.generator(out[:, -1])        # логиты для последнего токена\n        _, next_word = torch.max(prob, dim=1)     # выбираем индекс с максимальной вероятностью\n        next_word = next_word.item()\n        # Добавляем предсказанный токен к последовательности\n        ys = torch.cat([ys, torch.tensor([[next_word]], device=DEVICE)], dim=0)\n        if next_word == EOS_IDX:\n            break\n    return ys\n\ndef translate_sentence(model, sentence: str, tokenizer_src, tokenizer_tgt, max_len: int = 128):\n    model.eval()\n    # Токенизируем входное предложение (без добавления BOS/EOS, т.к. модель ожидает чистый текст)\n    src_tokens = tokenizer_src.encode(sentence)\n    src_tensor = torch.tensor(src_tokens, dtype=torch.long).unsqueeze(1).to(DEVICE)  # [seq_len, 1]\n    src_mask = torch.zeros(src_tensor.shape[0], src_tensor.shape[0], device=DEVICE).type(torch.bool)\n    # Выполняем greedy decode\n    tgt_tokens = greedy_decode(model, src_tensor, src_mask, max_len=max_len, start_symbol=BOS_IDX)\n    tgt_tokens = tgt_tokens.flatten().cpu().tolist()  # в список индексов\n    # Преобразуем последовательность индексов в текст, исключая BOS\n    # (SentencePiece.decode самостоятельно игнорирует специальные токены)\n    translated_text = tokenizer_tgt.decode(tgt_tokens)\n    return translated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:33:36.513583Z","iopub.execute_input":"2025-03-13T15:33:36.513930Z","iopub.status.idle":"2025-03-13T15:33:36.519694Z","shell.execute_reply.started":"2025-03-13T15:33:36.513899Z","shell.execute_reply":"2025-03-13T15:33:36.518901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate_sentence(model, sentence, tokenizer_src, tokenizer_tgt):\n    model.eval()\n\n    src_tokens = tokenizer_src.encode(sentence)\n    src_tensor = torch.tensor(src_tokens, dtype=torch.long).unsqueeze(1).to(DEVICE)\n\n    src_mask = torch.zeros(src_tensor.shape[0], src_tensor.shape[0], device=DEVICE).type(torch.bool)\n\n    tgt_tokens = greedy_decode(model, src_tensor, src_mask, max_len=128, start_symbol=2).flatten()\n\n    translated_sentence = tokenizer_tgt.decode(tgt_tokens.cpu().tolist())\n\n    return translated_sentence\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:25:56.601295Z","iopub.execute_input":"2025-03-13T14:25:56.601647Z","iopub.status.idle":"2025-03-13T14:25:56.606911Z","shell.execute_reply.started":"2025-03-13T14:25:56.601616Z","shell.execute_reply":"2025-03-13T14:25:56.605898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Загрузка обученных токенизаторов (если не сохранены ранее)\ntokenizer_de = spm.SentencePieceProcessor(model_file=\"tokenizer_german.model\")\ntokenizer_en = spm.SentencePieceProcessor(model_file=\"tokenizer_english.model\")\n\n# Пример перевода\nsentence_de = \"Eine Gruppe von Menschen steht vor einem Iglu.\"\ntranslation = translate_sentence(transformer, sentence_de, tokenizer_de, tokenizer_en)\nprint(\"🔹 Исходное (DE):\", sentence_de)\nprint(\"🔹 Перевод (EN):\", translation)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:33:39.388983Z","iopub.execute_input":"2025-03-13T15:33:39.389274Z","iopub.status.idle":"2025-03-13T15:33:39.520140Z","shell.execute_reply.started":"2025-03-13T15:33:39.389252Z","shell.execute_reply":"2025-03-13T15:33:39.519463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:40:16.140209Z","iopub.execute_input":"2025-03-13T15:40:16.140522Z","iopub.status.idle":"2025-03-13T15:46:51.007607Z","shell.execute_reply.started":"2025-03-13T15:40:16.140496Z","shell.execute_reply":"2025-03-13T15:46:51.006714Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}